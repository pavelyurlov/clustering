{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8de14cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from sklearn.datasets import make_blobs\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import pathlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c974daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(n_features, n_clusters, sigma=1, N=1000, repeat=20):\n",
    "    np.random.seed(n_features * n_clusters + N + repeat + 100 // sigma)\n",
    "    configuration_name = '{}x{}-{} {}'.format(N, n_features, n_clusters, sigma)\n",
    "    cluster_sizes = np.random.uniform(low=1, high=5, size=n_clusters)\n",
    "    cluster_sizes = (cluster_sizes * N / cluster_sizes.sum()).astype(int)\n",
    "    diff = cluster_sizes.sum() - N\n",
    "    cluster_sizes[0] -= diff\n",
    "\n",
    "    parent_path = './datasets/synthetic/{}'.format(configuration_name)\n",
    "    pathlib.Path(parent_path).mkdir(exist_ok=True)\n",
    "    for i in range(repeat):\n",
    "        np.random.seed(n_features * n_clusters + i * N)\n",
    "\n",
    "        child_path = '{}/{}'.format(parent_path, '%02d' % i)\n",
    "        pathlib.Path(child_path).mkdir(exist_ok=True)\n",
    "        \n",
    "        X = np.empty((N, n_features))\n",
    "        y = np.empty(N)\n",
    "        count_from, count_to = 0, 0\n",
    "        for c in range(n_clusters):\n",
    "            centre = np.random.normal(loc=0, scale=sigma, size=n_features)\n",
    "            std = np.sqrt(np.random.uniform(low=0.5, high=1.5, size=n_features))\n",
    "            count_to += cluster_sizes[c]\n",
    "            X[count_from:count_to, :] = np.random.normal(loc=centre, scale=std, size=(cluster_sizes[c], n_features))\n",
    "            y[count_from:count_to] = c\n",
    "            count_from += cluster_sizes[c]\n",
    "        \n",
    "        perm = np.random.permutation(len(y))\n",
    "        X = X[perm]\n",
    "        y = y[perm]\n",
    "        \n",
    "        np.savetxt('{}/X.csv'.format(child_path), X, fmt= '%.6f', delimiter=',')\n",
    "        np.savetxt('{}/y.csv'.format(child_path), y, fmt='%i', delimiter=',')\n",
    "        \n",
    "        del X, y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb95308d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_with_noise_features(n_features, n_noise_features, n_clusters, sigma=1, N=1000, repeat=20):\n",
    "    np.random.seed(n_features * n_clusters + N + repeat + 100 // sigma)\n",
    "    configuration_name = '{}x{}-{} +{}NF {}'.format(N, n_features, n_clusters, n_noise_features, sigma)\n",
    "    cluster_sizes = np.random.uniform(low=1, high=5, size=n_clusters)\n",
    "    cluster_sizes = (cluster_sizes * N / cluster_sizes.sum()).astype(int)\n",
    "    diff = cluster_sizes.sum() - N\n",
    "    cluster_sizes[0] -= diff\n",
    "\n",
    "    parent_path = './datasets/synthetic/{}'.format(configuration_name)\n",
    "    pathlib.Path(parent_path).mkdir(exist_ok=True)\n",
    "    for i in range(repeat):\n",
    "        np.random.seed(n_features * n_clusters + i * N)\n",
    "\n",
    "        child_path = '{}/{}'.format(parent_path, '%02d' % i)\n",
    "        pathlib.Path(child_path).mkdir(exist_ok=True)\n",
    "        \n",
    "        X = np.empty((N, n_features))\n",
    "        y = np.empty(N)\n",
    "        count_from, count_to = 0, 0\n",
    "        for c in range(n_clusters):\n",
    "            centre = np.random.normal(loc=0, scale=sigma, size=n_features)\n",
    "            std = np.sqrt(np.random.uniform(low=0.5, high=1.5, size=n_features))\n",
    "            count_to += cluster_sizes[c]\n",
    "            X[count_from:count_to, :] = np.random.normal(loc=centre, scale=std, size=(cluster_sizes[c], n_features))\n",
    "            y[count_from:count_to] = c\n",
    "            count_from += cluster_sizes[c]\n",
    "        \n",
    "        perm = np.random.permutation(len(y))\n",
    "        X = X[perm]\n",
    "        y = y[perm]\n",
    "        \n",
    "        # adding noise features\n",
    "        xmin, xmax = np.min(X), np.max(X)\n",
    "        X_noise = np.random.uniform(size=(N, n_noise_features), low=xmin, high=xmax)\n",
    "        X = np.hstack((X, X_noise))\n",
    "        # noise features added\n",
    "        \n",
    "        np.savetxt('{}/X.csv'.format(child_path), X, fmt= '%.6f', delimiter=',')\n",
    "        np.savetxt('{}/y.csv'.format(child_path), y, fmt='%i', delimiter=',')\n",
    "        \n",
    "        del X, X_noise, y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fc897b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_with_blurred_features(n_features, noise_percent, n_clusters, sigma=1, N=1000, repeat=20):\n",
    "    np.random.seed(n_features * n_clusters + N + repeat + 100 // sigma)\n",
    "    configuration_name = '{}x{}-{} {}%N {}'.format(N, n_features, n_clusters, noise_percent, sigma)\n",
    "    cluster_sizes = np.random.uniform(low=1, high=5, size=n_clusters)\n",
    "    cluster_sizes = (cluster_sizes * N / cluster_sizes.sum()).astype(int)\n",
    "    diff = cluster_sizes.sum() - N\n",
    "    cluster_sizes[0] -= diff\n",
    "\n",
    "    parent_path = './datasets/synthetic/{}'.format(configuration_name)\n",
    "    pathlib.Path(parent_path).mkdir(exist_ok=True)\n",
    "    for i in range(repeat):\n",
    "        np.random.seed(n_features * n_clusters + i * N)\n",
    "\n",
    "        child_path = '{}/{}'.format(parent_path, '%02d' % i)\n",
    "        pathlib.Path(child_path).mkdir(exist_ok=True)\n",
    "        \n",
    "        X = np.empty((N, n_features))\n",
    "        y = np.empty(N)\n",
    "        count_from, count_to = 0, 0\n",
    "        for c in range(n_clusters):\n",
    "            centre = np.random.normal(loc=0, scale=sigma, size=n_features)\n",
    "            std = np.sqrt(np.random.uniform(low=0.5, high=1.5, size=n_features))\n",
    "            count_to += cluster_sizes[c]\n",
    "            X[count_from:count_to, :] = np.random.normal(loc=centre, scale=std, size=(cluster_sizes[c], n_features))\n",
    "            y[count_from:count_to] = c\n",
    "            count_from += cluster_sizes[c]\n",
    "        \n",
    "        perm = np.random.permutation(len(y))\n",
    "        X = X[perm]\n",
    "        y = y[perm]\n",
    "        \n",
    "        # blurring features\n",
    "        x_mins, x_maxs = np.min(X, axis=0), np.max(X, axis=0)\n",
    "        feature_cluster_table = np.random.uniform(size=(n_features, n_clusters)) < noise_percent / 100\n",
    "        for f in range(n_features):\n",
    "            for c in range(n_clusters):\n",
    "                if feature_cluster_table[f, c]:\n",
    "                    mask = y == c\n",
    "                    noise = np.random.uniform(size=np.sum(mask), low=x_mins[f], high=x_maxs[f])\n",
    "                    X[mask, f] = noise\n",
    "        # features blurred\n",
    "        \n",
    "        np.savetxt('{}/X.csv'.format(child_path), X, fmt= '%.6f', delimiter=',')\n",
    "        np.savetxt('{}/y.csv'.format(child_path), y, fmt='%i', delimiter=',')\n",
    "        \n",
    "        del X, y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452d97eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898064dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7afbf37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmas = [1, 2, 3, 4, 5]\n",
    "feature_cluster_size = [(2, 3, 500), (6, 3, 1000), (12, 6, 1000), (20, 10, 1000)]\n",
    "\n",
    "for n_f, n_c, sz in feature_cluster_size:\n",
    "    for s in sigmas:\n",
    "        generate_data(n_f, n_c, s, N=sz)\n",
    "        generate_data_with_noise_features(n_f, n_f // 2, n_c, s, N=sz)\n",
    "        generate_data_with_blurred_features(n_f, 50, n_c, s, N=sz)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed317f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
